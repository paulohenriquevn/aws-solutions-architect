### Questão #201
Uma empresa está desenvolvendo um serviço de comunicações de marketing que tem como alvo os usuários de aplicativos móveis. A empresa precisa enviar mensagens de confirmação via Serviço de Mensagens Curtas (SMS) para seus usuários. Os usuários devem ser capazes de responder às mensagens SMS. A empresa deve armazenar as respostas por um ano para análise.

**Alternativas:**  
A. Criar um fluxo de contato do Amazon Connect para enviar as mensagens SMS. Usar o AWS Lambda para processar as respostas.  

B. Construir uma jornada no Amazon Pinpoint. Configurar o Amazon Pinpoint para enviar eventos para um fluxo de dados do Amazon Kinesis para análise e arquivamento.  

C. Usar o Amazon Simple Queue Service (Amazon SQS) para distribuir as mensagens SMS. Usar o AWS Lambda para processar as respostas.  

D. Criar um tópico FIFO do Amazon Simple Notification Service (Amazon SNS). Inscrever um fluxo de dados do Amazon Kinesis no tópico SNS para análise e arquivamento.

<details>
<summary>Resposta</summary>

**Resposta correta:**  
<resposta>B</resposta>  
**B:** Construir uma jornada no Amazon Pinpoint. Configurar o Amazon Pinpoint para enviar eventos para um fluxo de dados do Amazon Kinesis para análise e arquivamento.

**Justificativa:**  
- **Por que essa opção?**  
  O **Amazon Pinpoint** é um serviço projetado para gerenciar comunicações de marketing, incluindo o envio de mensagens SMS. Ele permite criar "jornadas" personalizadas que incluem envios de mensagens e o rastreamento de respostas. O envio de eventos para um fluxo de dados do Amazon Kinesis permite capturar, analisar e arquivar as respostas dos usuários, atendendo ao requisito de armazenamento por um ano.

- **Por que as outras opções não são adequadas?**  
  - **A:** O Amazon Connect é um serviço usado para gerenciar centros de contato, não sendo apropriado para gerenciar comunicações de marketing baseadas em SMS. Embora o Lambda possa processar respostas, essa solução não é otimizada para o caso de uso.  
  - **C:** O Amazon SQS é usado para filas de mensagens, mas não é adequado para enviar mensagens SMS diretamente ou gerenciar respostas. Ele requer integração adicional e complexa para alcançar os objetivos descritos.  
  - **D:** O Amazon SNS é um serviço de notificação e broadcast que não é projetado para lidar diretamente com respostas a mensagens SMS. Além disso, um tópico FIFO não é necessário neste caso e a integração com Kinesis não cobre a funcionalidade completa exigida para gerenciar respostas.

</details>

---

### Questão #202
Uma empresa está planejando mover seus dados para um bucket Amazon S3. Os dados devem ser criptografados quando armazenados no bucket S3. Além disso, a chave de criptografia deve ser rotacionada automaticamente todos os anos.

**Alternativas:**

A. Mova os dados para o bucket S3. Use a criptografia do lado do servidor com chaves de criptografia gerenciadas pelo Amazon S3 (SSE-S3). Use o comportamento integrado de rotação de chaves de criptografia do SSE-S3.

B. Crie uma chave gerenciada pelo cliente no AWS Key Management Service (AWS KMS). Habilite a rotação automática de chaves. Configure o comportamento padrão de criptografia do bucket S3 para usar a chave KMS gerenciada pelo cliente. Mova os dados para o bucket S3.

C. Crie uma chave gerenciada pelo cliente no AWS Key Management Service (AWS KMS). Configure o comportamento padrão de criptografia do bucket S3 para usar a chave KMS gerenciada pelo cliente. Mova os dados para o bucket S3. Faça a rotação manual da chave KMS todos os anos.

D. Criptografe os dados com material de chave do cliente antes de mover os dados para o bucket S3. Crie uma chave do AWS Key Management Service (AWS KMS) sem material de chave. Importe o material de chave do cliente para a chave KMS. Habilite a rotação automática de chaves.

<details>
<summary>Resposta</summary>

**Resposta correta:**  
<resposta>B</resposta>  
**B:** Crie uma chave gerenciada pelo cliente no AWS Key Management Service (AWS KMS). Habilite a rotação automática de chaves. Configure o comportamento padrão de criptografia do bucket S3 para usar a chave KMS gerenciada pelo cliente. Mova os dados para o bucket S3.

**Justificativa:**  
- **Por que essa opção?**  
  A alternativa B atende a todos os requisitos com o menor esforço operacional. As chaves gerenciadas pelo cliente no AWS KMS oferecem suporte à rotação automática de chaves anualmente. Configurar a criptografia padrão do bucket S3 para usar a chave KMS gerenciada pelo cliente garante que todos os dados armazenados no bucket sejam automaticamente criptografados sem a necessidade de etapas adicionais.

- **Por que as outras opções não são adequadas?**  
  - **A:** Embora o SSE-S3 ofereça criptografia no lado do servidor, ele não oferece suporte à rotação automática de chaves. Isso viola o requisito de rotação anual da chave.  
  - **C:** A rotação manual de chaves aumenta a sobrecarga operacional, o que não é consistente com o requisito de menor esforço operacional.  
  - **D:** Importar material de chave personalizado para o AWS KMS e gerenciar sua rotação manualmente é mais complexo e requer mais esforço operacional. Além disso, a rotação automática de chaves não é aplicável ao material de chave importado.

</details>

---

### Questão #203
Os clientes de uma empresa financeira solicitam agendamentos com consultores financeiros enviando mensagens de texto. Um aplicativo web que é executado em instâncias Amazon EC2 aceita as solicitações de agendamento. As mensagens de texto são publicadas em uma fila do Amazon Simple Queue Service (Amazon SQS) por meio do aplicativo web. Outro aplicativo, que também é executado em instâncias EC2, envia convites para reuniões e e-mails de confirmação de reuniões aos clientes. Após o agendamento bem-sucedido, este aplicativo armazena as informações das reuniões em um banco de dados Amazon DynamoDB.

Conforme a empresa se expande, os clientes relatam que os convites para reuniões estão demorando mais para chegar.

O que um arquiteto de soluções deve recomendar para resolver esse problema?

**Alternativas:**

A. Adicionar um cluster DynamoDB Accelerator (DAX) na frente do banco de dados DynamoDB.

B. Adicionar uma API do Amazon API Gateway na frente do aplicativo web que aceita as solicitações de agendamento.

C. Adicionar uma distribuição Amazon CloudFront. Definir a origem como o aplicativo web que aceita as solicitações de agendamento.

D. Adicionar um grupo Auto Scaling para o aplicativo que envia os convites para reuniões. Configurar o grupo Auto Scaling para escalar com base na profundidade da fila SQS.

<details>
<summary>Resposta</summary>

**Resposta correta:**  
<resposta>D</resposta>  
**D:** Adicionar um grupo Auto Scaling para o aplicativo que envia os convites para reuniões. Configurar o grupo Auto Scaling para escalar com base na profundidade da fila SQS.

**Justificativa:**  
- **Por que essa opção?**  
  A alternativa D é a melhor solução porque o atraso no envio dos convites é provavelmente causado por uma sobrecarga no aplicativo que processa as mensagens da fila SQS e envia os convites. Ao configurar um grupo Auto Scaling para escalar dinamicamente com base na profundidade da fila SQS, mais instâncias podem ser adicionadas para processar mensagens conforme necessário, reduzindo o atraso.

- **Por que as outras opções não são adequadas?**  
  - **A:** O DynamoDB Accelerator (DAX) melhora o desempenho de leituras no banco de dados DynamoDB, mas o problema relatado está relacionado ao envio de convites e não ao desempenho do banco de dados.  
  - **B:** Adicionar uma API do Amazon API Gateway na frente do aplicativo web não resolve o problema de atrasos no processamento da fila SQS e no envio de convites.  
  - **C:** Uma distribuição Amazon CloudFront acelera a entrega de conteúdo estático para os usuários finais, mas não está relacionada ao problema de processamento das mensagens na fila SQS ou ao envio dos convites.

</details>

---

### Questão #204
Uma empresa de varejo online possui mais de 50 milhões de clientes ativos e recebe mais de 25.000 pedidos por dia. A empresa coleta dados de compras dos clientes e armazena esses dados no Amazon S3. Dados adicionais dos clientes são armazenados no Amazon RDS.

A empresa deseja disponibilizar todos os dados para várias equipes para que possam realizar análises. A solução deve permitir o gerenciamento de permissões detalhadas para os dados e minimizar a sobrecarga operacional.

**Alternativas:**

A. Migrar os dados de compras para gravar diretamente no Amazon RDS. Usar os controles de acesso do RDS para limitar o acesso.

B. Agendar uma função AWS Lambda para copiar periodicamente os dados do Amazon RDS para o Amazon S3. Criar um crawler do AWS Glue. Usar o Amazon Athena para consultar os dados. Usar políticas do S3 para limitar o acesso.

C. Criar um data lake usando o AWS Lake Formation. Criar uma conexão JDBC do AWS Glue para o Amazon RDS. Registrar o bucket S3 no Lake Formation. Usar os controles de acesso do Lake Formation para limitar o acesso.

D. Criar um cluster Amazon Redshift. Agendar uma função AWS Lambda para copiar periodicamente dados do Amazon S3 e Amazon RDS para o Amazon Redshift. Usar os controles de acesso do Amazon Redshift para limitar o acesso.

<details>
<summary>Resposta</summary>

**Resposta correta:**  
<resposta>C</resposta>  
**C:** Criar um data lake usando o AWS Lake Formation. Criar uma conexão JDBC do AWS Glue para o Amazon RDS. Registrar o bucket S3 no Lake Formation. Usar os controles de acesso do Lake Formation para limitar o acesso.

**Justificativa:**  
- **Por que essa opção?**  
  O AWS Lake Formation é projetado para facilitar a criação e o gerenciamento de data lakes. Ele permite consolidar dados de diferentes fontes (como Amazon S3 e Amazon RDS), fornece controle detalhado de permissões e reduz significativamente a sobrecarga operacional ao automatizar tarefas complexas, como ingestão de dados e controle de acesso.  

- **Por que as outras opções não são adequadas?**  
  - **A:** Migrar todos os dados para o Amazon RDS criaria uma sobrecarga operacional significativa e limitaria a escalabilidade do sistema, além de não ser otimizado para grandes volumes de dados analíticos.  
  - **B:** Embora seja possível consultar os dados no Amazon S3 com o Amazon Athena, esta solução exige maior esforço manual para configurar e manter as permissões e o fluxo de dados entre o RDS e o S3.  
  - **D:** O Amazon Redshift é uma boa solução para análises de grandes volumes de dados, mas não oferece suporte direto a controles de acesso detalhados por equipe e exige uma configuração e manutenção mais complexas em comparação com o Lake Formation.

</details>

---

### Questão #205
Uma empresa hospeda um site de marketing em um data center local. O site consiste em documentos estáticos e é executado em um único servidor. Um administrador atualiza o conteúdo do site de forma infrequente e usa um cliente SFTP para carregar novos documentos.
A empresa decide hospedar seu site na AWS e usar o Amazon CloudFront. O arquiteto de soluções da empresa cria uma distribuição do CloudFront. O arquiteto deve projetar a arquitetura mais econômica e resiliente para hospedagem do site que servirá como origem para o CloudFront.

**Alternativas:**

A. Criar um servidor virtual usando o Amazon Lightsail. Configurar o servidor web na instância Lightsail. Carregar o conteúdo do site usando um cliente SFTP.

B. Criar um grupo de Auto Scaling do AWS para instâncias Amazon EC2. Usar um Application Load Balancer. Carregar o conteúdo do site usando um cliente SFTP.

C. Criar um bucket privado no Amazon S3. Usar uma política de bucket S3 para permitir acesso de uma identidade de acesso de origem (OAI) do CloudFront. Carregar o conteúdo do site usando o AWS CLI.

D. Criar um bucket público no Amazon S3. Configurar o AWS Transfer para SFTP. Configurar o bucket S3 para hospedagem de site. Carregar o conteúdo do site usando o cliente SFTP.

<details>
<summary>Resposta</summary>

**Resposta correta:**  
<resposta>C</resposta>  
**C:** Criar um bucket privado no Amazon S3. Usar uma política de bucket S3 para permitir acesso de uma identidade de acesso de origem (OAI) do CloudFront. Carregar o conteúdo do site usando o AWS CLI.

**Justificativa:**  
- **Por que essa opção?**  
  A opção C é a solução mais econômica e resiliente. O Amazon S3 é altamente disponível, escalável e ideal para hospedar conteúdos estáticos. Tornar o bucket privado e permitir o acesso apenas por meio do CloudFront com uma OAI garante segurança e controle sobre o acesso ao conteúdo. O AWS CLI permite um gerenciamento simples para carregar arquivos, reduzindo a necessidade de configurações adicionais.

- **Por que as outras opções não são adequadas?**  
  - **A:** O Amazon Lightsail pode ser uma solução simples, mas é menos resiliente do que o S3 para hospedar conteúdo estático e pode envolver mais custos operacionais a longo prazo.  
  - **B:** Configurar um Auto Scaling group com EC2 e um Application Load Balancer para hospedar conteúdo estático é desnecessário, caro e complexa para este caso de uso.  
  - **D:** Configurar o bucket S3 como público e usar o AWS Transfer para SFTP aumenta os custos e a exposição de segurança. Tornar o bucket público também vai contra as práticas recomendadas de segurança.

</details>

---

### Questão #206
Uma empresa deseja gerenciar Amazon Machine Images (AMIs). Atualmente, a empresa copia AMIs para a mesma região da AWS onde elas foram criadas. A empresa precisa projetar um aplicativo que capture chamadas de API da AWS e envie alertas sempre que a operação CreateImage da API do Amazon EC2 for chamada dentro da conta da empresa.

**Alternativas:**

A. Criar uma função AWS Lambda para consultar os logs do AWS CloudTrail e enviar um alerta quando uma chamada de API CreateImage for detectada.

B. Configurar o AWS CloudTrail com uma notificação do Amazon Simple Notification Service (Amazon SNS) que ocorre quando logs atualizados são enviados para o Amazon S3. Usar o Amazon Athena para criar uma nova tabela e consultar CreateImage quando uma chamada de API for detectada.

C. Criar uma regra do Amazon EventBridge (Amazon CloudWatch Events) para a chamada da API CreateImage. Configurar o destino como um tópico do Amazon Simple Notification Service (Amazon SNS) para enviar um alerta quando uma chamada de API CreateImage for detectada.

D. Configurar uma fila FIFO do Amazon Simple Queue Service (Amazon SQS) como destino para os logs do AWS CloudTrail. Criar uma função AWS Lambda para enviar um alerta para um tópico do Amazon Simple Notification Service (Amazon SNS) quando uma chamada de API CreateImage for detectada.

<details>
<summary>Resposta</summary>

**Resposta correta:**  
<resposta>C</resposta>  
**C:** Criar uma regra do Amazon EventBridge (Amazon CloudWatch Events) para a chamada da API CreateImage. Configurar o destino como um tópico do Amazon Simple Notification Service (Amazon SNS) para enviar um alerta quando uma chamada de API CreateImage for detectada.

**Justificativa:**  
- **Por que essa opção?**  
  O Amazon EventBridge oferece um meio direto e de baixa sobrecarga operacional para capturar eventos específicos, como a chamada da API CreateImage. Ele permite criar uma regra que monitora a chamada de API específica e envia notificações para um tópico SNS sem a necessidade de consultar logs manualmente ou configurar camadas adicionais de processamento.

- **Por que as outras opções não são adequadas?**  
  - **A:** Consultar os logs do AWS CloudTrail com uma função Lambda adiciona complexidade e maior sobrecarga operacional, pois requer a leitura frequente de logs.  
  - **B:** Usar o AWS Athena para consultar os logs do CloudTrail exige configuração e gerenciamento adicionais, além de ser mais lento do que a detecção em tempo real oferecida pelo EventBridge.  
  - **D:** Configurar uma fila SQS como destino para os logs do CloudTrail e usar uma função Lambda para processá-los aumenta significativamente a complexidade e a sobrecarga em comparação com uma regra direta do EventBridge.

</details>

---

### Questão #207
Uma empresa possui uma API assíncrona que é usada para processar solicitações de usuários e, com base no tipo de solicitação, encaminhá-las para o microserviço apropriado para processamento. A empresa usa o Amazon API Gateway para implantar o front-end da API e uma função AWS Lambda que invoca o Amazon DynamoDB para armazenar as solicitações de usuários antes de enviá-las para os microserviços de processamento.

A empresa provisionou a maior capacidade de throughput do DynamoDB que seu orçamento permite, mas ainda está enfrentando problemas de disponibilidade e perdendo solicitações de usuários.

O que um arquiteto de soluções deve fazer para resolver esse problema sem impactar os usuários existentes?

**Alternativas:**

A. Adicionar limitação (throttling) no API Gateway com limites de limitação no lado do servidor.

B. Usar o DynamoDB Accelerator (DAX) e o Lambda para armazenar em buffer as gravações no DynamoDB.

C. Criar um índice secundário no DynamoDB para a tabela com as solicitações dos usuários.

D. Usar uma fila do Amazon Simple Queue Service (Amazon SQS) e o Lambda para armazenar em buffer as gravações no DynamoDB.

<details>
<summary>Resposta</summary>

**Resposta correta:**  
<resposta>D</resposta>  
**D:** Usar uma fila do Amazon Simple Queue Service (Amazon SQS) e o Lambda para armazenar em buffer as gravações no DynamoDB.

**Justificativa:**  
- **Por que essa opção?**  
  A alternativa D resolve o problema ao introduzir um buffer com o Amazon SQS, que pode lidar com uma alta taxa de solicitações de entrada sem problemas de capacidade ou disponibilidade. As solicitações são armazenadas de forma confiável na fila e processadas pela função Lambda para gravar no DynamoDB em uma taxa que ele pode suportar. Isso evita a perda de solicitações e melhora a resiliência do sistema sem impactar os usuários existentes.

- **Por que as outras opções não são adequadas?**  
  - **A:** Limitar a taxa de solicitações no API Gateway reduz o número de solicitações processadas, mas não resolve o problema de gravações no DynamoDB. Isso pode impactar negativamente a experiência do usuário.  
  - **B:** O DynamoDB Accelerator (DAX) melhora a performance de leitura, mas não resolve problemas relacionados a gravações ou capacidade de throughput.  
  - **C:** Criar um índice secundário no DynamoDB não impacta diretamente o problema de throughput ou disponibilidade nas gravações, já que índices são usados principalmente para consultas adicionais.

</details>

---
### Questão #208
Uma empresa precisa mover dados de uma instância Amazon EC2 para um bucket Amazon S3. A empresa deve garantir que nenhuma chamada de API e nenhum dado sejam roteados através da internet pública. Somente a instância EC2 pode ter acesso para fazer o upload dos dados para o bucket S3.

**Alternativas:**

A. Criar um endpoint de interface VPC para o Amazon S3 na sub-rede onde a instância EC2 está localizada. Anexar uma política de recurso ao bucket S3 para permitir o acesso apenas ao papel (role) do IAM da instância EC2.

B. Criar um endpoint de gateway VPC para o Amazon S3 na zona de disponibilidade onde a instância EC2 está localizada. Anexar grupos de segurança apropriados ao endpoint. Anexar uma política de recurso ao bucket S3 para permitir o acesso apenas ao papel (role) do IAM da instância EC2.

C. Executar a ferramenta `nslookup` de dentro da instância EC2 para obter o endereço IP privado do endpoint da API de serviço do S3. Criar uma rota na tabela de rotas da VPC para fornecer à instância EC2 acesso ao bucket S3. Anexar uma política de recurso ao bucket S3 para permitir o acesso apenas ao papel (role) do IAM da instância EC2.

D. Usar o arquivo `ip-ranges.json` publicamente disponível fornecido pela AWS para obter o endereço IP privado do endpoint da API de serviço do S3. Criar uma rota na tabela de rotas da VPC para fornecer à instância EC2 acesso ao bucket S3. Anexar uma política de recurso ao bucket S3 para permitir o acesso apenas ao papel (role) do IAM da instância EC2.

<details>
<summary>Resposta</summary>

**Resposta correta:**  
<resposta>B</resposta>  
**B:** Criar um endpoint de gateway VPC para o Amazon S3 na zona de disponibilidade onde a instância EC2 está localizada. Anexar grupos de segurança apropriados ao endpoint. Anexar uma política de recurso ao bucket S3 para permitir o acesso apenas ao papel (role) do IAM da instância EC2.

**Justificativa:**  
- **Por que essa opção?**  
  Um endpoint de gateway VPC para o Amazon S3 permite que as instâncias EC2 acessem o S3 sem passar pela internet pública. Ele usa a rede privada da AWS para rotear chamadas de API para o bucket S3. Adicionar uma política de recurso no bucket garante que apenas a instância EC2 com o papel IAM especificado tenha acesso, atendendo a todos os requisitos de segurança e privacidade.

- **Por que as outras opções não são adequadas?**  
  - **A:** Um endpoint de interface VPC não é necessário para o S3, pois o gateway VPC é a solução recomendada e otimizada para este serviço.  
  - **C:** Usar `nslookup` para descobrir endereços IP não é uma prática confiável, já que os IPs dos serviços AWS podem mudar. Além disso, não garante que o tráfego não passe pela internet pública.  
  - **D:** O arquivo `ip-ranges.json` pode fornecer informações sobre os IPs, mas criar rotas manualmente não é eficiente nem confiável, pois os endereços IP podem mudar, e isso não garante que as conexões fiquem privadas.

</details>

---
### Questão #209
Um arquiteto de soluções está projetando a arquitetura de um novo aplicativo que será implantado na AWS Cloud. O aplicativo será executado em instâncias Amazon EC2 On-Demand e será escalado automaticamente em várias zonas de disponibilidade. As instâncias EC2 aumentarão e reduzirão frequentemente ao longo do dia. Um Application Load Balancer (ALB) será usado para distribuir a carga. A arquitetura precisa oferecer suporte ao gerenciamento de dados de sessão distribuídos. A empresa está disposta a fazer alterações no código, se necessário.

**Alternativas:**

A. Usar o Amazon ElastiCache para gerenciar e armazenar os dados de sessão.

B. Usar afinidade de sessão (sticky sessions) do ALB para gerenciar os dados de sessão.

C. Usar o Session Manager do AWS Systems Manager para gerenciar a sessão.

D. Usar a operação GetSessionToken da API do AWS Security Token Service (AWS STS) para gerenciar a sessão.

<details>
<summary>Resposta</summary>

**Resposta correta:**  
<resposta>A</resposta>  
**A:** Usar o Amazon ElastiCache para gerenciar e armazenar os dados de sessão.

**Justificativa:**  
- **Por que essa opção?**  
  O Amazon ElastiCache é uma solução escalável e de baixa latência para armazenar dados de sessão distribuídos em um cache de memória, como Redis ou Memcached. Isso permite que os dados de sessão sejam compartilhados entre várias instâncias EC2, mesmo que elas sejam escaladas dinamicamente. É uma prática recomendada para gerenciar dados de sessão em arquiteturas baseadas em nuvem.

- **Por que as outras opções não são adequadas?**  
  - **B:** Afinidade de sessão (sticky sessions) do ALB armazena dados de sessão no lado do cliente, mas isso pode causar problemas de escalabilidade e balanceamento de carga, especialmente quando as instâncias EC2 aumentam ou diminuem.  
  - **C:** O Session Manager do AWS Systems Manager é projetado para gerenciar sessões administrativas para acesso às instâncias EC2, não para o gerenciamento de dados de sessão de aplicativos.  
  - **D:** A operação GetSessionToken do AWS STS é usada para fornecer credenciais temporárias de segurança, não para gerenciar dados de sessão de usuários em aplicativos.

</details>

---

### Questão #210
Uma empresa oferece um serviço de entrega de alimentos que está crescendo rapidamente. Por causa desse crescimento, o sistema de processamento de pedidos da empresa está enfrentando problemas de escalabilidade durante as horas de pico. A arquitetura atual inclui o seguinte:

- Um grupo de instâncias Amazon EC2 em um Auto Scaling group para coletar pedidos do aplicativo.
- Outro grupo de instâncias EC2 em um Auto Scaling group para processar os pedidos.

O processo de coleta de pedidos ocorre rapidamente, mas o processo de processamento de pedidos pode levar mais tempo. Os dados não podem ser perdidos devido a eventos de escalabilidade.

Um arquiteto de soluções deve garantir que o processo de coleta de pedidos e o processo de processamento possam escalar adequadamente durante as horas de pico. A solução deve otimizar a utilização dos recursos AWS da empresa.

**Alternativas:**

A. Usar métricas do Amazon CloudWatch para monitorar a CPU de cada instância nos Auto Scaling groups. Configurar a capacidade mínima de cada Auto Scaling group de acordo com os valores de carga máxima.

B. Usar métricas do Amazon CloudWatch para monitorar a CPU de cada instância nos Auto Scaling groups. Configurar um alarme do CloudWatch para invocar um tópico do Amazon Simple Notification Service (Amazon SNS) que cria Auto Scaling groups adicionais sob demanda.

C. Provisionar duas filas do Amazon Simple Queue Service (Amazon SQS): uma para coleta de pedidos e outra para processamento de pedidos. Configurar as instâncias EC2 para consultar suas respectivas filas. Escalar os Auto Scaling groups com base em notificações enviadas pelas filas.

D. Provisionar duas filas do Amazon Simple Queue Service (Amazon SQS): uma para coleta de pedidos e outra para processamento de pedidos. Configurar as instâncias EC2 para consultar suas respectivas filas. Criar uma métrica baseada em uma calculadora de backlog por instância. Escalar os Auto Scaling groups com base nessa métrica.

<details>
<summary>Resposta</summary>

**Resposta correta:**  
<resposta>D</resposta>  
**D:** Provisionar duas filas do Amazon Simple Queue Service (Amazon SQS): uma para coleta de pedidos e outra para processamento de pedidos. Configurar as instâncias EC2 para consultar suas respectivas filas. Criar uma métrica baseada em uma calculadora de backlog por instância. Escalar os Auto Scaling groups com base nessa métrica.

**Justificativa:**  
- **Por que essa opção?**  
  A alternativa D é a mais eficiente e resiliente porque permite desacoplar o processo de coleta de pedidos do processo de processamento. Usar filas SQS permite armazenar pedidos de forma confiável até que possam ser processados, mesmo durante eventos de pico. Criar uma métrica de backlog por instância garante que a escalabilidade dos Auto Scaling groups seja proporcional à carga de trabalho, otimizando a utilização dos recursos.

- **Por que as outras opções não são adequadas?**  
  - **A:** Escalar com base na capacidade mínima configurada para cargas de pico pode resultar em superprovisionamento durante horários de baixa demanda, aumentando os custos.  
  - **B:** Criar Auto Scaling groups adicionais com base em alarmes do CloudWatch não resolve o problema de desacoplamento entre a coleta e o processamento de pedidos, e pode levar a uma solução mais complexa e difícil de gerenciar.  
  - **C:** Escalar com base em notificações das filas é uma abordagem menos eficiente do que calcular o backlog por instância, pois não leva em consideração a capacidade de processamento das instâncias EC2.

</details>

---

### Questão #211
Uma empresa hospeda múltiplas aplicações em produção. Uma dessas aplicações consiste em recursos do Amazon EC2, AWS Lambda, Amazon RDS, Amazon Simple Notification Service (Amazon SNS) e Amazon Simple Queue Service (Amazon SQS) distribuídos em várias regiões da AWS. Todos os recursos da empresa são marcados com a tag "application" e um valor correspondente a cada aplicação. Um arquiteto de soluções deve fornecer a solução mais rápida para identificar todos os componentes marcados.

**Alternativas:**

A. Usar o AWS CloudTrail para gerar uma lista de recursos com a tag "application".

B. Usar o AWS CLI para consultar cada serviço em todas as regiões e gerar um relatório dos componentes marcados.

C. Executar uma consulta no Amazon CloudWatch Logs Insights para relatar os componentes com a tag "application".

D. Executar uma consulta no AWS Resource Groups Tag Editor para gerar um relatório global dos recursos com a tag "application".

<details>
<summary>Resposta</summary>

**Resposta correta:**  
<resposta>D</resposta>  
**D:** Executar uma consulta no AWS Resource Groups Tag Editor para gerar um relatório global dos recursos com a tag "application".

**Justificativa:**  
- **Por que essa opção?**  
  O AWS Resource Groups Tag Editor é projetado especificamente para gerenciar e consultar tags em recursos AWS. Ele oferece uma interface e funcionalidades para buscar rapidamente recursos por tags em várias regiões e serviços. É a maneira mais eficiente de identificar todos os componentes marcados com a tag "application" de forma global.

- **Por que as outras opções não são adequadas?**  
  - **A:** O AWS CloudTrail é usado para rastrear atividades e chamadas de API, mas não é uma solução eficiente ou direta para listar recursos baseados em tags.  
  - **B:** Consultar manualmente cada serviço em todas as regiões usando o AWS CLI seria demorado e exigiria automação adicional para alcançar o mesmo resultado que o Tag Editor fornece de forma nativa.  
  - **C:** O Amazon CloudWatch Logs Insights é usado para analisar logs, não para buscar recursos com base em tags. Isso seria inadequado para esse caso de uso.

</details>

---
### Questão #212
Uma empresa precisa exportar seu banco de dados uma vez por dia para o Amazon S3 para que outras equipes possam acessá-lo. O tamanho do objeto exportado varia entre 2 GB e 5 GB. O padrão de acesso ao S3 é variável e muda rapidamente. Os dados devem estar imediatamente disponíveis e permanecer acessíveis por até 3 meses. A empresa precisa da solução mais econômica que não aumente o tempo de recuperação.

**Alternativas:**

A. S3 Intelligent-Tiering

B. S3 Glacier Instant Retrieval

C. S3 Standard

D. S3 Standard-Infrequent Access (S3 Standard-IA)

<details>
<summary>Resposta</summary>

**Resposta correta:**  
<resposta>A</resposta>  
**A:** S3 Intelligent-Tiering

**Justificativa:**  
- **Por que essa opção?**  
  O S3 Intelligent-Tiering é a solução mais econômica para padrões de acesso imprevisíveis e variáveis. Ele automaticamente move os objetos entre diferentes camadas de armazenamento com base nos padrões de acesso, mantendo os dados imediatamente disponíveis quando necessário. Isso reduz os custos sem comprometer o tempo de recuperação, atendendo perfeitamente às necessidades da empresa.

- **Por que as outras opções não são adequadas?**  
  - **B:** O S3 Glacier Instant Retrieval é mais econômico, mas é projetado para dados acessados raramente e pode não atender a requisitos de acesso altamente variáveis.  
  - **C:** O S3 Standard oferece alta disponibilidade e baixa latência, mas é mais caro do que o Intelligent-Tiering para padrões de acesso imprevisíveis.  
  - **D:** O S3 Standard-Infrequent Access (S3 Standard-IA) é mais barato do que o S3 Standard, mas é adequado apenas para dados acessados com menos frequência. O custo de recuperação pode ser maior se os dados forem acessados frequentemente, tornando-o inadequado para padrões de acesso variáveis.

</details>

---
### Questão #213
Uma empresa está desenvolvendo um novo aplicativo móvel. A empresa deve implementar um filtro de tráfego adequado para proteger seu Application Load Balancer (ALB) contra ataques comuns em nível de aplicação, como cross-site scripting (XSS) ou SQL injection. A empresa possui equipe mínima de infraestrutura e operações e precisa reduzir sua responsabilidade no gerenciamento, atualização e segurança de servidores em seu ambiente AWS.

**Alternativas:**

A. Configurar regras do AWS WAF e associá-las ao ALB.

B. Implantar o aplicativo usando o Amazon S3 com hospedagem pública habilitada.

C. Implantar o AWS Shield Advanced e adicionar o ALB como um recurso protegido.

D. Criar um novo ALB que direcione o tráfego para uma instância Amazon EC2 executando um firewall de terceiros, que então passa o tráfego para o ALB atual.

<details>
<summary>Resposta</summary>

**Resposta correta:**  
<resposta>A</resposta>  
**A:** Configurar regras do AWS WAF e associá-las ao ALB.

**Justificativa:**  
- **Por que essa opção?**  
  O AWS WAF (Web Application Firewall) é uma solução gerenciada pela AWS que protege contra ataques comuns em nível de aplicação, como XSS e SQL injection. Ele pode ser associado diretamente ao ALB, reduzindo a necessidade de gerenciar servidores adicionais ou configurar soluções complexas. Além disso, o AWS WAF é fácil de configurar, atende aos requisitos da empresa e reduz sua responsabilidade operacional.

- **Por que as outras opções não são adequadas?**  
  - **B:** Implantar o aplicativo no Amazon S3 com hospedagem pública habilitada não protege contra ataques em nível de aplicação e expõe diretamente o aplicativo à internet pública, o que é inseguro.  
  - **C:** O AWS Shield Advanced protege contra ataques DDoS, mas não é projetado para lidar com ataques em nível de aplicação, como XSS ou SQL injection.  
  - **D:** Criar um novo ALB e usar um firewall de terceiros em uma instância EC2 adiciona complexidade e sobrecarga operacional, o que não é consistente com o objetivo de reduzir a responsabilidade operacional da empresa.

</details>

---
### Questão #214
O sistema de relatórios de uma empresa entrega centenas de arquivos `.csv` para um bucket Amazon S3 todos os dias. A empresa precisa converter esses arquivos para o formato Apache Parquet e armazená-los em um bucket de dados transformados.

**Alternativas:**

A. Criar um cluster Amazon EMR com o Apache Spark instalado. Escrever uma aplicação Spark para transformar os dados. Usar o EMR File System (EMRFS) para gravar os arquivos no bucket de dados transformados.

B. Criar um crawler do AWS Glue para descobrir os dados. Criar um trabalho ETL (extract, transform, load) no AWS Glue para transformar os dados. Especificar o bucket de dados transformados na etapa de saída.

C. Usar o AWS Batch para criar uma definição de trabalho com sintaxe Bash para transformar os dados e enviar os dados para o bucket de dados transformados. Usar a definição de trabalho para enviar um trabalho. Especificar um trabalho de array como tipo de trabalho.

D. Criar uma função AWS Lambda para transformar os dados e enviar os dados para o bucket de dados transformados. Configurar uma notificação de evento para o bucket S3. Especificar a função Lambda como o destino para a notificação de evento.

<details>
<summary>Resposta</summary>

**Resposta correta:**  
<resposta>B</resposta>  
**B:** Criar um crawler do AWS Glue para descobrir os dados. Criar um trabalho ETL (extract, transform, load) no AWS Glue para transformar os dados. Especificar o bucket de dados transformados na etapa de saída.

**Justificativa:**  
- **Por que essa opção?**  
  O AWS Glue é uma solução gerenciada que oferece suporte para tarefas ETL com o mínimo de esforço de desenvolvimento. Ele automatiza a descoberta de esquemas por meio do crawler e permite criar trabalhos ETL para transformar os arquivos `.csv` em Parquet. O Glue lida com a transformação e grava os arquivos diretamente no bucket de destino, reduzindo significativamente o esforço de configuração e manutenção.

- **Por que as outras opções não são adequadas?**  
  - **A:** Criar um cluster EMR e escrever um aplicativo Spark exige mais esforço de desenvolvimento e manutenção em comparação com o AWS Glue.  
  - **C:** O AWS Batch é mais adequado para processamentos paralelos complexos e não é otimizado para pipelines de transformação de dados estruturados como este caso.  
  - **D:** Usar uma função Lambda seria mais adequado para arquivos menores e cargas de trabalho leves. No entanto, transformar centenas de arquivos `.csv` diariamente pode exceder os limites de tempo de execução ou tamanho de payload do Lambda, tornando-o uma escolha ineficaz.

</details>

---
### Questão #215
Uma empresa possui 700 TB de dados de backup armazenados em um armazenamento conectado em rede (NAS) em seu data center. Esses dados de backup precisam ser acessíveis para solicitações regulatórias infrequentes e devem ser retidos por 7 anos. A empresa decidiu migrar esses dados para a AWS. A migração deve ser concluída em 1 mês. A empresa possui 500 Mbps de largura de banda dedicada em sua conexão de internet pública disponível para a transferência de dados.

**Alternativas:**

A. Solicitar dispositivos AWS Snowball para transferir os dados. Usar uma política de ciclo de vida para transicionar os arquivos para o Amazon S3 Glacier Deep Archive.

B. Implantar uma conexão VPN entre o data center e a Amazon VPC. Usar o AWS CLI para copiar os dados do local para o Amazon S3 Glacier.

C. Provisionar uma conexão AWS Direct Connect de 500 Mbps e transferir os dados para o Amazon S3. Usar uma política de ciclo de vida para transicionar os arquivos para o Amazon S3 Glacier Deep Archive.

D. Usar o AWS DataSync para transferir os dados e implantar um agente DataSync no local. Usar a tarefa DataSync para copiar arquivos do armazenamento NAS local para o Amazon S3 Glacier.

<details>
<summary>Resposta</summary>

**Resposta correta:**  
<resposta>A</resposta>  
**A:** Solicitar dispositivos AWS Snowball para transferir os dados. Usar uma política de ciclo de vida para transicionar os arquivos para o Amazon S3 Glacier Deep Archive.

**Justificativa:**  
- **Por que essa opção?**  
  O AWS Snowball é a solução mais econômica e eficiente para transferir grandes volumes de dados (700 TB) dentro do prazo exigido (1 mês). Com 500 Mbps de largura de banda, seria impossível transferir 700 TB de dados dentro do prazo. Snowball permite transferir dados em dispositivos físicos, eliminando a dependência de largura de banda da internet. Uma vez na AWS, uma política de ciclo de vida pode ser usada para mover os dados para o S3 Glacier Deep Archive, que é a opção de armazenamento de menor custo para retenção de longo prazo.

- **Por que as outras opções não são adequadas?**  
  - **B:** Usar VPN e AWS CLI para transferir os dados através da conexão de 500 Mbps levaria mais de 1 mês para concluir, mesmo com largura de banda totalmente dedicada.  
  - **C:** Embora o Direct Connect seja mais rápido do que a internet pública, provisionar e configurar uma conexão Direct Connect leva tempo significativo, o que torna improvável a conclusão da migração dentro do prazo de 1 mês.  
  - **D:** O AWS DataSync é eficaz para volumes menores de dados, mas a limitação da largura de banda de 500 Mbps impossibilitaria a conclusão da transferência dentro do prazo exigido.

</details>

---
### Questão #216
Uma empresa tem um site serverless com milhões de objetos em um bucket Amazon S3. O bucket S3 é usado como origem para uma distribuição do Amazon CloudFront. A empresa não ativou a criptografia no bucket S3 antes de carregar os objetos. Um arquiteto de soluções precisa habilitar a criptografia para todos os objetos existentes e para todos os objetos adicionados ao bucket S3 no futuro.

**Alternativas:**

A. Criar um novo bucket S3. Ativar as configurações de criptografia padrão para o novo bucket S3. Fazer download de todos os objetos existentes para um armazenamento local temporário. Fazer upload dos objetos para o novo bucket S3.

B. Ativar as configurações de criptografia padrão para o bucket S3. Usar o recurso S3 Inventory para criar um arquivo `.csv` que lista os objetos não criptografados. Executar um trabalho do S3 Batch Operations que usa o comando `copy` para criptografar esses objetos.

C. Criar uma nova chave de criptografia usando o AWS Key Management Service (AWS KMS). Alterar as configurações do bucket S3 para usar criptografia do lado do servidor com chaves gerenciadas pelo AWS KMS (SSE-KMS). Ativar o versionamento para o bucket S3.

D. Navegar até o console do Amazon S3. Procurar pelos objetos no bucket S3. Ordenar pelo campo de criptografia. Selecionar cada objeto não criptografado. Usar o botão **Modificar** para aplicar as configurações de criptografia padrão a cada objeto não criptografado no bucket S3.

<details>
<summary>Resposta</summary>

**Resposta correta:**  
<resposta>B</resposta>  
**B:** Ativar as configurações de criptografia padrão para o bucket S3. Usar o recurso S3 Inventory para criar um arquivo `.csv` que lista os objetos não criptografados. Executar um trabalho do S3 Batch Operations que usa o comando `copy` para criptografar esses objetos.

**Justificativa:**  
- **Por que essa opção?**  
  A opção B permite ativar a criptografia padrão para todos os objetos futuros com um único ajuste de configuração no bucket S3. Para os objetos existentes, o S3 Inventory pode listar os objetos não criptografados, e o S3 Batch Operations pode aplicar a criptografia copiando os objetos para si mesmos. Essa abordagem é automatizada e eficiente para lidar com milhões de objetos.

- **Por que as outras opções não são adequadas?**  
  - **A:** Fazer download e upload manual de milhões de objetos é demorado, caro e requer muito esforço de desenvolvimento e recursos locais.  
  - **C:** Alterar para SSE-KMS e ativar o versionamento adiciona custos desnecessários e não resolve o problema dos objetos existentes sem esforço adicional para reescrever os dados.  
  - **D:** Navegar manualmente e aplicar a criptografia objeto por objeto no console S3 é impraticável e não escalável para milhões de objetos.

</details>

---
### Questão #217
Uma empresa executa um aplicativo web global em instâncias Amazon EC2 atrás de um Application Load Balancer. O aplicativo armazena dados no Amazon Aurora. A empresa precisa criar uma solução de recuperação de desastres e pode tolerar até 30 minutos de tempo de inatividade e perda potencial de dados. A solução não precisa suportar a carga quando a infraestrutura primária está saudável.

**Alternativas:**

A. Implantar o aplicativo com os elementos de infraestrutura necessários no lugar. Usar o Amazon Route 53 para configurar failover ativo-passivo. Criar uma Aurora Replica em uma segunda região da AWS.

B. Hospedar uma implantação reduzida do aplicativo em uma segunda região da AWS. Usar o Amazon Route 53 para configurar failover ativo-ativo. Criar uma Aurora Replica na segunda região.

C. Replicar a infraestrutura primária em uma segunda região da AWS. Usar o Amazon Route 53 para configurar failover ativo-ativo. Criar um banco de dados Aurora restaurado a partir do snapshot mais recente.

D. Fazer backup dos dados com o AWS Backup. Usar o backup para criar a infraestrutura necessária em uma segunda região da AWS. Usar o Amazon Route 53 para configurar failover ativo-passivo. Criar uma segunda instância primária Aurora na segunda região.

<details>
<summary>Resposta</summary>

**Resposta correta:**  
<resposta>A</resposta>  
**A:** Implantar o aplicativo com os elementos de infraestrutura necessários no lugar. Usar o Amazon Route 53 para configurar failover ativo-passivo. Criar uma Aurora Replica em uma segunda região da AWS.

**Justificativa:**  
- **Por que essa opção?**  
  A configuração de failover ativo-passivo usando o Amazon Route 53 permite redirecionar o tráfego para a segunda região em caso de falha na infraestrutura primária. A Aurora Replica em uma segunda região fornece redundância para os dados, permitindo recuperação rápida com potencial perda mínima de dados, dentro do limite de 30 minutos. Esta abordagem é econômica, já que a infraestrutura secundária não precisa estar ativa enquanto a primária está saudável.

- **Por que as outras opções não são adequadas?**  
  - **B:** O failover ativo-ativo exige que a infraestrutura secundária esteja dimensionada para lidar com a carga de produção, o que é mais caro e desnecessário neste caso, já que a solução não precisa suportar carga em tempo normal.  
  - **C:** Restaurar um banco de dados Aurora a partir de um snapshot mais recente pode levar mais de 30 minutos, o que não atende ao requisito de tolerância de downtime. Além disso, a replicação ativa-ativa aumenta os custos desnecessariamente.  
  - **D:** Usar backups e criar infraestrutura sob demanda na segunda região é mais lento do que usar uma Aurora Replica configurada previamente, excedendo a janela de tolerância ao downtime.

</details>

---

### Questão #218
Uma empresa possui um servidor web executando em uma instância Amazon EC2 em uma sub-rede pública com um endereço IP elástico. O grupo de segurança padrão está associado à instância EC2. A ACL de rede padrão foi modificada para bloquear todo o tráfego. Um arquiteto de soluções precisa tornar o servidor web acessível de qualquer lugar na porta 443. Escolha duas

**Alternativas:**

A. Criar um grupo de segurança com uma regra para permitir TCP na porta 443 a partir da origem 0.0.0.0/0.

B. Criar um grupo de segurança com uma regra para permitir TCP na porta 443 para o destino 0.0.0.0/0.

C. Atualizar a ACL de rede para permitir TCP na porta 443 a partir da origem 0.0.0.0/0.

D. Atualizar a ACL de rede para permitir TCP de entrada/saída na porta 443 da origem 0.0.0.0/0 para o destino 0.0.0.0/0.

E. Atualizar a ACL de rede para permitir TCP de entrada na porta 443 a partir da origem 0.0.0.0/0 e TCP de saída na faixa de portas 32768-65535 para o destino 0.0.0.0/0.

<details>
<summary>Resposta</summary>

**Respostas corretas:**  
<resposta>A</resposta>  
<resposta>E</resposta>  

**A:** Criar um grupo de segurança com uma regra para permitir TCP na porta 443 a partir da origem 0.0.0.0/0.  
**E:** Atualizar a ACL de rede para permitir TCP de entrada na porta 443 a partir da origem 0.0.0.0/0 e TCP de saída na faixa de portas 32768-65535 para o destino 0.0.0.0/0.

**Justificativa:**  
- **Por que essas opções?**  
  - A opção **A** garante que o grupo de segurança da instância EC2 permita conexões na porta 443 de qualquer lugar (0.0.0.0/0).  
  - A opção **E** garante que a ACL de rede permita o tráfego necessário:  
    - Entrada na porta 443 para receber solicitações HTTPS.  
    - Saída na faixa de portas 32768-65535, necessária para conexões de resposta.  

- **Por que as outras opções não são adequadas?**  
  - **B:** Regras de grupo de segurança não especificam destinos; elas controlam o tráfego de entrada baseado na origem e o tráfego de saída baseado no destino.  
  - **C:** Permitir apenas o tráfego de entrada na ACL não é suficiente, pois o tráfego de saída também precisa ser permitido para responder às solicitações.  
  - **D:** Permitir tráfego de entrada e saída na porta 443 para todos os destinos é desnecessário. Apenas a faixa de saída 32768-65535 é necessária para conexões de resposta.

</details>

---

### Questão #219
O aplicativo de uma empresa está enfrentando problemas de desempenho. O aplicativo é stateful e precisa concluir tarefas na memória em instâncias Amazon EC2. A empresa utilizou AWS CloudFormation para implantar a infraestrutura e usou a família de instâncias EC2 M5. À medida que o tráfego aumentou, o desempenho do aplicativo diminuiu, e os usuários estão relatando atrasos ao tentar acessar o aplicativo.
Qual solução resolverá esses problemas da maneira mais eficiente operacionalmente?

**Alternativas:**

A. Substituir as instâncias EC2 por instâncias T3 que executam em um Auto Scaling group. Fazer as alterações usando o AWS Management Console.

B. Modificar os templates do CloudFormation para executar as instâncias EC2 em um Auto Scaling group. Aumentar manualmente a capacidade desejada e a capacidade máxima do Auto Scaling group quando necessário.

C. Modificar os templates do CloudFormation. Substituir as instâncias EC2 por instâncias R5. Usar as métricas de memória integradas do Amazon CloudWatch para monitorar o desempenho do aplicativo para planejamento de capacidade futuro.

D. Modificar os templates do CloudFormation. Substituir as instâncias EC2 por instâncias R5. Implantar o agente Amazon CloudWatch nas instâncias EC2 para gerar métricas personalizadas de latência do aplicativo para planejamento de capacidade futuro.

<details>
<summary>Resposta</summary>

**Resposta correta:**  
<resposta>D</resposta>  
**D:** Modificar os templates do CloudFormation. Substituir as instâncias EC2 por instâncias R5. Implantar o agente Amazon CloudWatch nas instâncias EC2 para gerar métricas personalizadas de latência do aplicativo para planejamento de capacidade futuro.

**Justificativa:**  
- **Por que essa opção?**  
  A família de instâncias R5 é otimizada para aplicativos que exigem alto desempenho em memória, como o aplicativo descrito. Substituir as instâncias M5 por R5 atenderá melhor às necessidades de capacidade de memória do aplicativo. Implantar o agente CloudWatch para gerar métricas personalizadas de latência ajuda a monitorar e ajustar o desempenho do aplicativo de forma proativa, garantindo uma operação eficiente e suporte para futuras expansões.

- **Por que as outras opções não são adequadas?**  
  - **A:** As instâncias T3 são projetadas para workloads de burst temporário e não têm capacidade suficiente para atender a requisitos de memória elevados e consistentes.  
  - **B:** A introdução de um Auto Scaling group sem abordar o problema de capacidade de memória subjacente não resolverá os problemas de desempenho.  
  - **C:** Embora as instâncias R5 atendam às necessidades de memória, usar apenas métricas de memória padrão do CloudWatch não é suficiente para monitorar problemas de latência do aplicativo.

</details>

---
### Questão #220
Um arquiteto de soluções está projetando uma nova API usando o Amazon API Gateway que receberá solicitações dos usuários. O volume de solicitações é altamente variável; podem passar várias horas sem receber uma única solicitação. O processamento dos dados ocorrerá de forma assíncrona, mas deve ser concluído dentro de alguns segundos após uma solicitação ser feita.

**Alternativas:**

A. Um trabalho do AWS Glue.

B. Uma função AWS Lambda.

C. Um serviço conteinerizado hospedado no Amazon Elastic Kubernetes Service (Amazon EKS).

D. Um serviço conteinerizado hospedado no Amazon ECS com Amazon EC2.

<details>
<summary>Resposta</summary>

**Resposta correta:**  
<resposta>B</resposta>  
**B:** Uma função AWS Lambda.

**Justificativa:**  
- **Por que essa opção?**  
  O AWS Lambda é ideal para cargas de trabalho event-driven, especialmente para cenários com padrões de uso variáveis e intermitentes, como descrito na questão. Ele é baseado em um modelo serverless e escala automaticamente, o que elimina custos operacionais quando não há solicitações. Além disso, o Lambda oferece baixa latência e é capaz de processar tarefas rapidamente, atendendo ao requisito de concluir o processamento em poucos segundos.

- **Por que as outras opções não são adequadas?**  
  - **A:** O AWS Glue é projetado para tarefas ETL (Extract, Transform, Load) de grandes volumes de dados, e sua inicialização pode levar minutos, o que não atende ao requisito de processamento em segundos.  
  - **C:** Hospedar um serviço em Amazon EKS envolve custos contínuos com o cluster Kubernetes, mesmo quando não há solicitações, o que torna esta solução mais cara e menos eficiente para cargas de trabalho intermitentes.  
  - **D:** O Amazon ECS com instâncias EC2 também gera custos contínuos, já que as instâncias permanecem ativas, independentemente do tráfego, aumentando os custos operacionais em relação ao AWS Lambda.

</details>

---
### Questão #221
Uma empresa executa um aplicativo em um grupo de instâncias Amazon Linux EC2. Por razões de conformidade, a empresa deve reter todos os arquivos de log do aplicativo por 7 anos. Os arquivos de log serão analisados por uma ferramenta de relatórios que deve ser capaz de acessar todos os arquivos simultaneamente.

**Alternativas:**

A. Amazon Elastic Block Store (Amazon EBS)

B. Amazon Elastic File System (Amazon EFS)

C. Amazon EC2 instance store

D. Amazon S3

<details>
<summary>Resposta</summary>

**Resposta correta:**  
<resposta>D</resposta>  
**D:** Amazon S3

**Justificativa:**  
- **Por que essa opção?**  
  O Amazon S3 é uma solução de armazenamento altamente durável e escalável, ideal para retenção de longo prazo e acessos simultâneos. Ele oferece integração com ferramentas de análise, suporta retenção de 7 anos com políticas de ciclo de vida e é a solução mais econômica para armazenar grandes volumes de logs. Além disso, o S3 permite acessos simultâneos por vários consumidores sem impacto no desempenho.

- **Por que as outras opções não são adequadas?**  
  - **A:** O Amazon EBS é projetado para armazenamento de dados em bloco diretamente anexado a uma instância EC2. Ele não é ideal para armazenamento de longo prazo, já que exige que a instância EC2 permaneça ativa, o que aumenta os custos.  
  - **B:** O Amazon EFS é adequado para compartilhamento de arquivos entre múltiplas instâncias, mas é mais caro que o S3 para retenção de longo prazo de arquivos que não precisam de acesso frequente.  
  - **C:** O Amazon EC2 instance store é armazenamento efêmero que não persiste após a interrupção da instância, o que o torna inadequado para retenção de logs por 7 anos.

</details>

---
### Questão #222
Uma empresa contratou um fornecedor externo para realizar trabalho na conta AWS da empresa. O fornecedor usa uma ferramenta automatizada que está hospedada em uma conta AWS pertencente ao fornecedor. O fornecedor não tem acesso IAM à conta AWS da empresa.

**Alternativas:**

A. Criar uma função IAM na conta da empresa para delegar acesso à função IAM do fornecedor. Anexar as políticas IAM apropriadas à função para as permissões que o fornecedor precisa.

B. Criar um usuário IAM na conta da empresa com uma senha que atenda aos requisitos de complexidade de senha. Anexar as políticas IAM apropriadas ao usuário para as permissões que o fornecedor precisa.

C. Criar um grupo IAM na conta da empresa. Adicionar o usuário IAM da ferramenta da conta do fornecedor ao grupo. Anexar as políticas IAM apropriadas ao grupo para as permissões que o fornecedor precisa.

D. Criar um novo provedor de identidade escolhendo "conta AWS" como o tipo de provedor no console IAM. Fornecer o ID da conta AWS e o nome de usuário do fornecedor. Anexar as políticas IAM apropriadas ao novo provedor para as permissões que o fornecedor precisa.

<details>
<summary>Resposta</summary>

**Resposta correta:**  
<resposta>A</resposta>  
**A:** Criar uma função IAM na conta da empresa para delegar acesso à função IAM do fornecedor. Anexar as políticas IAM apropriadas à função para as permissões que o fornecedor precisa.

**Justificativa:**  
- **Por que essa opção?**  
  Delegar acesso por meio de uma função IAM é a abordagem recomendada para cenários intercontas na AWS. A empresa pode criar uma função IAM com confiança para a conta do fornecedor, permitindo que a ferramenta automatizada assuma a função para executar tarefas específicas. Isso é seguro e fácil de gerenciar, já que as permissões podem ser estritamente controladas com políticas IAM e não requerem credenciais compartilhadas.

- **Por que as outras opções não são adequadas?**  
  - **B:** Criar um usuário IAM para o fornecedor exige compartilhar credenciais, o que é menos seguro e não segue as melhores práticas de segurança da AWS.  
  - **C:** Usuários IAM não podem ser adicionados a grupos de outras contas AWS, tornando esta opção inválida.  
  - **D:** Criar um provedor de identidade do tipo "conta AWS" não é aplicável neste caso, pois este tipo de provedor não suporta integração com funções intercontas diretamente.

</details>

---
### Questão #223
Uma empresa implantou um aplicativo Java Spring Boot como um pod que é executado no Amazon Elastic Kubernetes Service (Amazon EKS) em sub-redes privadas. O aplicativo precisa gravar dados em uma tabela do Amazon DynamoDB. Um arquiteto de soluções deve garantir que o aplicativo possa interagir com a tabela DynamoDB sem expor o tráfego à internet. Escolha duas.

**Alternativas:**

A. Anexar uma função IAM com privilégios suficientes ao pod do EKS.

B. Anexar um usuário IAM com privilégios suficientes ao pod do EKS.

C. Permitir conectividade de saída para a tabela DynamoDB por meio das ACLs de rede das sub-redes privadas.

D. Criar um endpoint VPC para o DynamoDB.

E. Incorporar as chaves de acesso no código do Java Spring Boot.

<details>
<summary>Resposta</summary>

**Respostas corretas:**  
<resposta>A</resposta>  
<resposta>D</resposta>  

**A:** Anexar uma função IAM com privilégios suficientes ao pod do EKS.  
**D:** Criar um endpoint VPC para o DynamoDB.

**Justificativa:**  
- **Por que essas opções?**  
  - **A:** Anexar uma função IAM ao pod do EKS (usando o AWS IAM Roles for Service Accounts - IRSA) permite que o pod tenha permissões temporárias para interagir com o DynamoDB sem a necessidade de chaves de acesso embutidas no código, seguindo as melhores práticas de segurança.  
  - **D:** Criar um endpoint VPC para o DynamoDB permite que o tráfego entre o aplicativo e o DynamoDB seja roteado pela rede privada da AWS, eliminando a necessidade de expor o tráfego à internet.  

- **Por que as outras opções não são adequadas?**  
  - **B:** Anexar um usuário IAM diretamente ao pod não é possível e, mesmo que fosse, é menos seguro do que usar uma função IAM.  
  - **C:** Embora permitir conectividade de saída por meio das ACLs seja necessário, isso sozinho não resolve o problema de evitar o tráfego pela internet.  
  - **E:** Incorporar chaves de acesso no código viola as melhores práticas de segurança da AWS, pois expõe as credenciais e aumenta o risco de comprometimento.

</details>

---
### Questão #224
Uma empresa recentemente migrou seu aplicativo web para a AWS, reimplantando o aplicativo em instâncias Amazon EC2 em uma única região da AWS. A empresa deseja redesenhar a arquitetura do aplicativo para ser altamente disponível e tolerante a falhas. O tráfego deve alcançar todas as instâncias EC2 em execução de forma aleatória. Escolha duas.

**Alternativas:**

A. Criar uma política de roteamento de failover no Amazon Route 53.

B. Criar uma política de roteamento ponderado no Amazon Route 53.

C. Criar uma política de roteamento de resposta múltipla no Amazon Route 53.

D. Iniciar três instâncias EC2: duas instâncias em uma zona de disponibilidade e uma instância em outra zona de disponibilidade.

E. Iniciar quatro instâncias EC2: duas instâncias em uma zona de disponibilidade e duas instâncias em outra zona de disponibilidade.

<details>
<summary>Resposta</summary>

**Respostas corretas:**  
<resposta>C</resposta>  
<resposta>E</resposta>  

**C:** Criar uma política de roteamento de resposta múltipla no Amazon Route 53.  
**E:** Iniciar quatro instâncias EC2: duas instâncias em uma zona de disponibilidade e duas instâncias em outra zona de disponibilidade.

**Justificativa:**  
- **Por que essas opções?**  
  - **C:** Uma política de roteamento de resposta múltipla no Amazon Route 53 permite distribuir o tráfego de forma aleatória para várias instâncias EC2. Isso atende ao requisito de distribuir o tráfego para todas as instâncias em execução.  
  - **E:** Implantar instâncias EC2 em múltiplas zonas de disponibilidade garante alta disponibilidade e tolerância a falhas, já que a aplicação permanece funcional mesmo se uma zona de disponibilidade apresentar problemas.  

- **Por que as outras opções não são adequadas?**  
  - **A:** Uma política de failover no Route 53 é usada para direcionar o tráfego para um recurso de backup quando o principal falha, o que não atende ao requisito de distribuir o tráfego aleatoriamente.  
  - **B:** Uma política de roteamento ponderado no Route 53 distribui o tráfego com base em pesos específicos, mas não garante distribuição aleatória para todas as instâncias.  
  - **D:** Usar apenas três instâncias com uma distribuição desigual entre zonas de disponibilidade reduz a tolerância a falhas e não fornece alta disponibilidade ideal.

</details>

---
### Questão #225
Uma empresa de mídia coleta e analisa dados de atividades dos usuários em um ambiente local. A empresa deseja migrar essa funcionalidade para a AWS. O armazenamento de dados de atividades continuará a crescer e atingirá tamanhos na ordem de petabytes. A empresa precisa construir uma solução de ingestão de dados altamente disponível que facilite análises sob demanda de dados existentes e novos usando SQL.

**Alternativas:**

A. Enviar os dados de atividades para um stream de dados do Amazon Kinesis. Configurar o stream para entregar os dados para um bucket Amazon S3.

B. Enviar os dados de atividades para um stream de entrega do Amazon Kinesis Data Firehose. Configurar o stream para entregar os dados para um cluster Amazon Redshift.

C. Colocar os dados de atividades em um bucket Amazon S3. Configurar o Amazon S3 para executar uma função AWS Lambda nos dados à medida que chegam no bucket S3.

D. Criar um serviço de ingestão em instâncias Amazon EC2 distribuídas por várias zonas de disponibilidade. Configurar o serviço para encaminhar dados para um banco de dados Amazon RDS Multi-AZ.

<details>
<summary>Resposta</summary>

**Resposta correta:**  
<resposta>B</resposta>  
**B:** Enviar os dados de atividades para um stream de entrega do Amazon Kinesis Data Firehose. Configurar o stream para entregar os dados para um cluster Amazon Redshift.

**Justificativa:**  
- **Por que essa opção?**  
  O Kinesis Data Firehose é uma solução gerenciada e de baixo overhead para ingestão de dados. Ele entrega automaticamente os dados para o Amazon Redshift, que é altamente otimizado para análises SQL em grandes volumes de dados. Essa solução combina alta disponibilidade, escalabilidade e suporte para análises sob demanda, sem exigir configurações operacionais complexas.

- **Por que as outras opções não são adequadas?**  
  - **A:** Embora o Kinesis Data Streams possa entregar dados para o S3, ele requer etapas adicionais para processar e analisar os dados com SQL, o que aumenta a sobrecarga operacional.  
  - **C:** Executar uma função Lambda nos dados à medida que chegam no S3 não facilita análises SQL diretamente e não é ideal para grandes volumes de dados devido às limitações do Lambda.  
  - **D:** Um serviço de ingestão personalizado com Amazon EC2 e Amazon RDS envolve alta sobrecarga operacional e não é otimizado para análise em escala de petabytes.

</details>

---
